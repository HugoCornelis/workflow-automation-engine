#!/usr/bin/perl -w
#!/usr/bin/perl -d:ptkdb
#

use strict;


my $loaded_mail_sender = eval "require Mail::Sender";

use Data::Transformator;

use Expect;

use Getopt::Long;

use Neurospaces::Tester;;

use YAML;


my $option_check_test_names;
my $option_dump_json;
my $option_dump_perl;
my $option_dump_yaml;
my $option_email = defined $ENV{NEUROSPACES_HARNESS_OPTION_EMAIL} ? $ENV{NEUROSPACES_HARNESS_OPTION_EMAIL} : 0 ;
my $option_flattenout;
my $option_help;
my $option_numerical_compare;
my $option_output;
my $option_output_html;
my $option_output_latex;
my $option_output_levels = [];
my $option_random_order;
my $option_regex_selector = ".*";
my $option_report_disabled;
my $option_show;
my $option_show_command_tests;
my $option_tags = [];
my $option_timeout_multiplier = 1;
my $option_timings;
my $option_trace;
my $option_verbose;


our $global_config;

my $global_all_output_levels
    = [
       "meta",
       "module",
       "command_definition",
       "command_test",
      ];

my $global_error_count = 0;
my $global_fd_output;
my $global_previous_command_side_effects;
my $global_test_report;


$SIG{'__DIE__'}
    = sub
      {
	  use Carp;

	  print STDERR Carp::longmess(@_);

	  $global_test_report->{global}->{status} = 'Died';

	  report_exit(3, @_);
      };


$SIG{'INT'}
    = sub
      {
	  $global_test_report->{global}->{status} = 'Interrupted';

	  report_exit(2);
      };


sub initialize_config_environment
{
    my $fd_initialization;

    open($fd_initialization, ">&STDOUT");

    my $environment_settings = $global_config->{environment_settings};

    foreach my $environment_setting_name (keys %$environment_settings)
    {
	my $environment_setting = $environment_settings->{$environment_setting_name};

	# if simply setting values in the environment

	if (!ref $environment_setting)
	{
	    # set the value in the environment

	    $ENV{$environment_setting_name} = $environment_setting;
	}

	# if additional initialization required

	elsif (ref $environment_setting eq 'HASH')
	{
	    # get environment_setting value

	    my $value = $environment_setting->{value};

	    # set the value in the environment

	    $ENV{$environment_setting_name} = $value;

	    # give feedback

	    my $description = $environment_setting->{description};

	    $description =~ s/%value/$value/g;

	    if ($option_verbose)
	    {
		print $fd_initialization $description;
	    }

	    # loop over shell commands for initialization

	    my $shell_commands = $environment_setting->{initialization}->{shell};

	    foreach my $shell_command (@$shell_commands)
	    {
		# replace value

		$shell_command =~ s/%value/$value/g;

		# execute command

		if ($option_verbose)
		{
		    print $fd_initialization "$0: executing ($shell_command)\n";
		}

		system $shell_command;
	    }
	}
	else
	{
	    die "$0: illegal environment_settings, was processing $environment_setting";
	}
    }
}


sub initialize_globals
{
    $global_test_report
	= {
	   %$global_test_report,
	   description => {
			   command => $0,
			   name => "Test report",
			   package => $global_config->{package},
			  },
	   global => {
		      config => $global_config,
		      error_count => $global_error_count,
		      status => 'initializing',
		      test_counters => {
					module_definition => 0,
					command_definition => 0,
					command_test => 0,
				       },
		      time_start => `date`,
		     },
	   target => {
		      OS => $^O,
		      system => {
				 libc => (join '', `(ls -l /lib/libc-* && ls -l /usr/lib/libc-*) 2>/dev/null`),
				 uname => (join '', `uname -a`),
				 cpu => (join '', `cat /proc/cpuinfo`),
				},
		      packages => {
				   '/etc/lsb-release' => (join '', `cat /etc/lsb-release`),
				   autoconf => (join '', `autoconf --version`),
				   automake => (join '', `automake --version`),
				   bison => (join '', `bison --version`),
				   flex => (join '', `flex --version`),
				   gcc => (join '', `gcc 2>&1 --version && gcc 2>&1 -v`),
				   perl => (join '', `perl 2>&1 -v && perl 2>&1 -V`),
				   python => (join '', `python 2>&1 --version`),
				   python_installation => (join '', `./contrib/pythontest 2>&1`),
				   swig => (join '', `swig 2>&1 -version`),
				  },
		     },
	  };
}


sub initialize_config
{
    # find the package core directory

    $global_config = do './tests.config';

    if (!defined $global_config)
    {
	die "No test configuration found";
    }

    # add to tests directory to include paths

    if (!exists $global_config->{tests_directory})
    {
	$global_config->{tests_directory} = "./tests/specifications";

	$global_config->{introduction_filename} = "tests/introduction.html";
    }

    unshift @INC, $global_config->{tests_directory};
}


sub main
{
    parse_options();

    initialize_config();

    initialize_config_environment();

    initialize_globals();

    open($global_fd_output, ">&STDOUT");

    # find all the test module files

    my $test_module_references = Heterarch::Test::Library::construct();

    # read the test module files

    my $test_module_library_contents = Heterarch::Test::Library::expand($global_fd_output, $test_module_references);

    my $test_modules = $test_module_library_contents->{test_modules};

    # compute the model library checksum

    my $library_sha_before;

    if (defined ($global_config->{model_library}))
    {
	$library_sha_before = ModelLibrary::sha();
    }

    # the previous command had no side effects on the loaded model (since there is no model yet)

    $global_previous_command_side_effects = 0;

    # set status: running

    $global_test_report->{global}->{status} = 'Running';

    # randomize test module order if requested

    if (defined $option_random_order)
    {
	if ($option_verbose)
	{
	    print $global_fd_output "Randomizing test specifications, random seed is $option_random_order\n";
	}

	# by default the randomizer will choose a random_seed

	my $random_seed;

	if ($option_random_order =~ /^[0-9]*$/
	    and $option_random_order ne 1)
	{
	    $random_seed = $option_random_order
	}

	# randomize the test module library

	( $test_modules, $random_seed ) = Heterarch::Test::Library::randomize_order($test_modules, $random_seed);

	$global_test_report->{random_seed} = $random_seed;
    }
    else
    {
	$global_test_report->{random_seed} = 'not randomized';
    }

    $global_test_report->{global}->{test_counters}
	= {
	   module_definition => 0,
	   command_definition => 0,
	   command_test => 0,
	  };

    # create an execution context for running the tests

    my $execution_context = Heterarch::Test::ExecutionContext->new($global_fd_output, { library_sha_before => $library_sha_before, }, );

    # create a test executor for running the tests following the requested type of output

    my $executor = $option_output->new($execution_context);

    # loop over all module definitions

    foreach my $module_definition (@$test_modules)
    {
	bless $module_definition, "Heterarch::Test::Module";

	my $fd_module_output = $global_fd_output;

	my $error = $module_definition->md_run($execution_context, $executor, $fd_module_output);

	# increment module definition test count

	$global_test_report->{global}->{test_counters}->{module_definition}++;
    }

    # end the execution_context

    $execution_context->terminate();

    # close pending commands

    Heterarch::Test::CommandDefinition::Interactive::close_pending();

    # close the test report

    $global_test_report->{global}->{status} = 'Finished';

    # final report

    report_exit(0, $test_module_library_contents);
}


sub parse_options
{
    $option_numerical_compare = $global_config->{numerical_compare} || 0;

    my $result
	= GetOptions
	    (
	     "check-test-names!" => \$option_check_test_names,
	     "dump-json!" => \$option_dump_json,
	     "dump-perl!" => \$option_dump_perl,
	     "dump-yaml!" => \$option_dump_yaml,
	     "email" => \$option_email,
	     "flattenout" => \$option_flattenout,
	     "help!" => \$option_help,
	     "numerical-compare!" => \$option_numerical_compare,
	     "output=s" => \$option_output,
	     "output-html" => \$option_output_html,
	     "output-latex" => \$option_output_latex,
	     "output-levels=s" => $option_output_levels,
	     "randomize-order=s" => \$option_random_order,
	     "regex-selector=s" => \$option_regex_selector,
	     "report-disabled!" => \$option_report_disabled,
	     "show!" => \$option_show,
	     "show-command-tests!" => \$option_show_command_tests,
	     "tags=s" => $option_tags,
	     "timeout-multiplier=s" => \$option_timeout_multiplier,
	     "timings!" => \$option_timings,
	     "trace!" => \$option_trace,
	     "v|verbose+" => \$option_verbose,
	    );

    if (!$result)
    {
	die "$0: Error in option processing";
    }

    if (not defined $option_output)
    {
	if ($option_output_html)
	{
	    $global_test_report->{output_format} = 'yaml_exit';

	    $option_output = "Heterarch::Test::Output::Formatter::HTMLTable";
	}
	elsif ($option_output_latex)
	{
	    $global_test_report->{output_format} = 'latex';

	    $option_output = "Heterarch::Test::Output::Formatter::Latex";
	}
	elsif ($option_show)
	{
	    $global_test_report->{output_format} = 'yaml_exit';

	    $option_output = "Heterarch::Test::Output::Formatter::YAML::Summary";
	}
	elsif ($option_show_command_tests)
	{
	    $global_test_report->{output_format} = 'yaml_exit';

	    $option_output = "Heterarch::Test::Output::Formatter::YAML::CommandTests";
	}
	else
	{
	    $global_test_report->{output_format} = 'executor';

	    $option_output = "Heterarch::Test::Executor";
	}
    }

    if (not scalar @$option_output_levels)
    {
	use Clone 'clone';

	$option_output_levels = clone($global_all_output_levels);
    }

    $global_test_report->{options}
	= {
	   dump_json => $option_dump_json,
	   dump_perl => $option_dump_perl,
	   dump_yaml => $option_dump_yaml,
	   email => $option_email,
	   flattenout => $option_flattenout,
	   help => $option_help,
	   numerical_compare => $option_numerical_compare,
	   output => $option_output,
	   output_html => $option_output_html,
	   output_latex => $option_output_latex,
	   output_levels => $option_output_levels,
	   random_order => $option_random_order,
	   regex_selector => $option_regex_selector,
	   report_disabled => $option_report_disabled,
	   show => $option_show,
	   show_command_tests => $option_show_command_tests,
	   tags => $option_tags,
	   timeout_multiplier => $option_timeout_multiplier,
	   timings => $option_timings,
	   trace => $option_trace,
	   verbose => $option_verbose,
	  };

    if ($option_help)
    {
	print
	    "
$0: test definition executor

options:
    --check-test-names   check the pathnames of the specification files against the names of the tests.
    --dump-json          dump test specifications to json files in /tmp/.
    --dump-perl          dump test specifications to perl files in /tmp/.
    --dump-yaml          dump test specifications to yaml files in /tmp/.
    --email              allow to send emails, the default is taken from \$ENV{NEUROSPACES_HARNESS_OPTION_EMAIL}.
    --flattenout         flattenout the test definitions before testing,
                         this experimental feature might increase test performance,
                         by recycling test definitions.
    --help               print usage information.
    --numerical-compare  attempt to compare numbers numerically when string differences are found (default enabled).
    --output             the class that should generate the output (default is '$option_output').
    --output-html        generate HTML output.
    --output-latex       generate Latex output.
    --output-levels      generate output for these levels (default is '" . ( join ', ', @$global_all_output_levels ) . "'.
    --randomize-order    randomize the order of the tests before executing them (require List::Util to be installed).
    --regex-selector     defines a regex to run specific tests.
    --report-disabled    include information of disabled tests in the test report.
    --show               show tests that would be run using the current configuration.
    --show-command-tests show all the command definitions that would be run during test execution, including the input to those commands.
    --tags               only test test modules that have been tagged with these tags (default: all tags).
    --timeout-multiplier multiply all timeout values with this constant.
    --timings            add timing information about the tests to the report.
    --trace              enable tracing using the strace unix shell command.
    --verbose            set verbosity level.
";

	exit 1;
    }

}


sub report_exit
{
    my $exit_code = shift;

    my $description = shift;

    $global_test_report->{global}->{time_end} = `date`;

    if (defined $description
        && $exit_code eq 3)
    {
	print STDERR "*** die: $description\n";
    }

    # yaml out the test report

    my $report_yaml
	= {
	   description => $global_test_report->{description},
	  };

    # add output specific to the output selection

    if ($option_report_disabled)
    {
	$report_yaml->{disabled} = $global_test_report->{disabled};
    }

    # add output specific to the output_format

    if ($global_test_report->{output_format} eq 'executor')
    {
	$report_yaml->{errors} = $global_test_report->{errors};
	$report_yaml->{global} = $global_test_report->{global};
    }
    elsif ($global_test_report->{output_format} eq 'yaml_exit')
    {
	$report_yaml->{selected} = $global_test_report->{selected};
    }

    # generate ascii text based on the content of the output

    my $report_text = "\n" . Dump($report_yaml);

    if ($global_test_report->{output_format} eq 'executor')
    {
	print $global_fd_output "$report_text\n";
    }
    elsif ($global_test_report->{output_format} eq 'yaml_exit')
    {
	print $global_fd_output "$report_text\n";
    }
    elsif ($global_test_report->{output_format} eq 'latex')
    {
	my $test_module_library_contents = ref($description) eq 'HASH' ? $description : {};

	my $selected_output_levels = $Heterarch::Test::Element::selected_output_levels;

	my $extension = join '-', sort keys %$selected_output_levels;

	my $output_filename = "/tmp/test_report_$global_config->{package}->{name}_$extension.tex";

	Heterarch::Test::Output::Formatter::Latex::process
		(
		 $output_filename,
		 $test_module_library_contents,
		 $global_test_report,
		);

	my $testing = 1;

	if ($testing)
	{
	    my $output_filename = "/tmp/test_report_$global_config->{package}->{name}.tex";

	    Heterarch::Test::Output::Formatter::Latex::process
		    (
		     $output_filename,
		     $test_module_library_contents,
		     $global_test_report,
		    );
	}
    }

    # and write the full report to a file

    my $report_filename = ">/tmp/report_$global_config->{package}->{name}.yml";

    eval
    {
	YAML::DumpFile($report_filename, $global_test_report);
    };

    if ($@)
    {
	print STDERR "*** Error: Failed to write output report to $report_filename\n";
    }
    else
    {
	print $global_fd_output "*** Info: See '$report_filename' for the detailed test report\n";
    }

    # if email enabled by the options

    if ($option_email)
    {
	# check for a default route

	#! from perl/basic.t

	my $no_default_route = (`/sbin/route` =~ /default/ ? '' : 'no default route to the internet found');

	if (!$no_default_route)
	{
	    # ask if sending an email is ok

	    my $default_answer_prompt = $global_error_count ? "Y/n" : "y/N";

	    print $global_fd_output "\n---\n  These tests generated $global_error_count error(s)";
	    print $global_fd_output ( $global_error_count ? "  Because errors were found, you should really consider sending an email to inform the developer of this software package\n" : "" );
	    print $global_fd_output "\n  Ok to send an email to hugo.cornelis\@gmail.com for this test report,\n  this email will not reveal your identity to the recipient ?  [$default_answer_prompt]";

	    my $answer = readline(*STDIN);

	    # send an email if ok

	    if ($answer =~ /^y/)
	    {
		#t perhaps should consider Mail::Builder, don't know

		if ($loaded_mail_sender)
		{
		    my $sender
			= Mail::Sender->new
			    (
			     {
# 			      smtp => 'googlemail.l.google.com',
			      smtp => 'mta1.uthscsa.edu',
			      from => 'hugo.cornelis@gmail.com',
			     },
			    );

		    print $global_fd_output "Sending ... should not take more than 30 seconds
";

		    my $message
			= (
			   "neurospaces_harness test report\n"
			   . "Generated on " . `date` . "\n"
			   . "\n========\n"
			   . Dump($global_test_report)
			   . "\n========\n"
			  );

		    $sender->MailMsg
			(
			 {
			  to => 'cornelis@uthscsa.edu',
			  subject => '[neurospaces_harness] test report',
			  msg => $message,
			 },
			);

		    # 	    $sender->Attach
		    # 		(
		    # 		 {
		    # 		  description => 'global_test_report',
		    # 		 },
		    # 		);

		    $sender->Close();

		    if ($sender->{error})
		    {
			print STDERR
			    "*** Error: $sender->{error}
Email was not sent.
";
		    }
		    else
		    {
			print $global_fd_output
			    "Email was sent.
";
		    }
		}
		else
		{
		    print STDERR
			"*** Error: Cannot load the perl module Mail::Sender, contact your sysadmin to install this perl module.
(use the shell command \"sudo perl -MCPAN -e 'install Mail::Sender'\")
Email was not sent.
";
		}
	    }
	    else
	    {
		print $global_fd_output
		    "Email was not sent.
";
	    }
	}
	else
	{
	    print STDERR "*** Warning: No default route to the internet found.
Email was not sent.
";
	}
    }
    else
    {
	print $global_fd_output "No email sent.
";
    }

    # if there were errors

    if ($global_error_count)
    {
	# exit with failure

	$exit_code ||= 1;

	print $global_fd_output "$0: $global_test_report->{global}->{test_counters}->{command_test} test(s), $global_error_count error(s)\n";

	print $global_fd_output "$0: exit_code $exit_code\n\n";

	exit $exit_code;
    }

    # else

    else
    {
	# exit, possibly success

	print $global_fd_output "$0: $global_error_count error(s)\n\n";

	exit $exit_code;
    }
}


package ModelLibrary;


sub sha
{
    # find all models

    use File::Find::Rule;

    my $files = [ File::Find::Rule->file()->in( $ENV{NEUROSPACES_NMC_MODELS} ), ];

    my $shas
	= [
	   map
	   {
	       `sha1sum $_`;
	   }
	   @$files,
	  ];

    use Digest::SHA qw(sha1_hex);

    my $sha = sha1_hex(join ', ', @$shas);

    return $sha;
}


package Heterarch::Test::CommandDefinition;


our @ISA = ("Heterarch::Test::Element");


sub rebless
{
    my $command_definition = shift;

    my $target_package = "Heterarch::Test::CommandDefinition";

    # we have to either instantiate an object that produces output ...

    if ($command_definition->{class})
    {
	$target_package = "Heterarch::Test::CommandDefinition::PerlClass";
    }

    # ... or run perl code that produces output

    elsif (ref $command_definition->{command} eq 'CODE')
    {
	$target_package = "Heterarch::Test::CommandDefinition::PerlCode";
    }

    # ... or run a command that produces output

    else
    {
	$target_package = "Heterarch::Test::CommandDefinition::Interactive";
    }

    # rebless

    bless $command_definition, $target_package;

    # return result

    return $command_definition;
}


package Heterarch::Test::CommandDefinition::Interactive;;


our @ISA = ("Heterarch::Test::CommandDefinition");


use Data::Comparator qw(data_comparator);


my $global_exp;

my $global_running_command_definition;


sub close_pending
{
    # if there is a command running

    if ($global_exp)
    {
	# terminate the command

	#t could be that the hard_close() call is needed because
	#t neurospaces uses readline, not sure needs investigation,
	#t perhaps.

	$global_exp->hard_close();
    }
}


sub spawn_new
{
    my $command_definition = shift;

    my $fd_output = shift;

    # a command can be an absolute pathname,
    # can come from the _build directory
    # or from the sources for a script

# 		-f $command and print "Found command $command\n";
# 		-f $built_exe_directory . $command and print "Found built $built_exe_directory$command\n";
# 		-f $core_directory . $command and print "Found core $core_directory$command\n";

    my $command = $command_definition->{command};

    my $test_command = $command;
    # = (-f $command
    #    ? $command
    #    : (defined $built_exe_directory
    #       ? (-f $built_exe_directory . $command
    #        ? $built_exe_directory . $command
    #        : $core_directory . $command)
    #       : $command));

    # find differences between how to run this command and
    # the one already running

    my $new_command_definition
	= {
	    arguments => $command_definition->{arguments},
	    command => $test_command,
	  };

    my $differences = data_comparator($new_command_definition, $global_running_command_definition);

    # remember to spawn a new command

    my $spawn_new;

    $spawn_new = not $command_definition->{recycle};

    # (
	#  # if we do not have a previous command

	#  (not $global_exp)

	#  # or if there were differences with the previous command

	#  || (not $differences->is_empty())

	#  # or if the previous command had side effects

	#  || $global_previous_command_side_effects

	#  # or if we had to prepare this command

	#  || $command_definition->{preparation}
	# )
	# && (not $command_definition->{recycle});

    # prefix the command with the core directory

    $command = $new_command_definition->{command};

    my $arguments = $command_definition->{arguments};

    # if the command can be executed

    my $test_startup;

# 		    if (-x $command)
    if (defined $command)
    {
	if ($spawn_new)
	{
	    if ($global_exp)
	    {
		# terminate the previous command

		#t could be that the hard_close() call is needed because
		#t neurospaces uses readline, not sure needs investigation,
		#t perhaps.

		$global_exp->hard_close();
	    }

	    # create a new Expect object by spawning a new process with the new command

	    $global_exp = Expect->new();

	    #! see the expect manual for this one

	    $global_exp->raw_pty(1);

	    # 		    $exp->slave->stty(qw(raw -echo));

	    $global_exp->spawn
		(
		 (
		  $option_trace
		  ? ("strace", "-f")
		  : ()
		 ),
		 $command,
		 @$arguments
		)
		or die "$0: cannot spawn $command: $!\n";

	    # set the running_command_definition

	    $global_running_command_definition = $new_command_definition;

	    print $fd_output "*** Executing $command " . (join ' ', map { "'$_'" } @$arguments) . "\n";

	    # remember to do startup testing

	    $test_startup = 1;

	    # there were no side effects yet

	    $global_previous_command_side_effects = 0;
	}
	else
	{
	    print $fd_output "*** Recycling $command " . (join ' ', map { "'$_'" } @$arguments) . "\n";

	    $test_startup = 0;
	}
    }
    # else
    # {
    # 	my $error = "$command is not executable";

    # 	Heterarch::Test::Reporting::report_error_add
    # 	    (
    # 	     {
    # 	      description => $description,
    # 	      error => $error,
    # 	      module_name => $module_name,
    # 	     },
    # 	    );

    # 	next;
    # }

    return $global_exp, $test_startup;
}


#
# sub start
#
# Starts the application command and connects to its I/O channels.
#

sub cd_start
{
    my $self = shift;

    my $executor = shift;

    my $fd_output = shift;

    # start the command and connect with its I/O channels

    #! this is similar to having a class clause with value 'Expect' and calling Expect->new()

    my ($exp, $test_startup) = $self->spawn_new($fd_output);

    $executor->{expect_object} = $exp;

    #! I believe this is obsolete

    $executor->{test_startup} = $test_startup;
}


package Heterarch::Test::CommandDefinition::PerlClass;;


our @ISA = ("Heterarch::Test::CommandDefinition");


#
# sub start
#
# Calls the new method of the given class to instantiate a Perl object.
#

sub cd_start
{
    my $self = shift;

    my $executor = shift;

    my $fd_output = shift;

    # instantiate the object

    my $class = $self->{class};

    my $filename = $class . ".pm";

    $filename =~ s(::)(/)g;

    require $filename;

    $self->{command_object} = eval "$class->new()";
}


package Heterarch::Test::CommandDefinition::PerlCode;;


our @ISA = ("Heterarch::Test::CommandDefinition");


#
# sub start
#
# No-op.
#
# Deprecated: Hardcoded perl code in the test specifications should be removed.
#

sub cd_start
{
    my $self = shift;

    my $executor = shift;

    my $fd_output = shift;

    # code is already instantiated, nothing to do here
}


package Heterarch::Test::CommandTest;


our @ISA = ("Heterarch::Test::Element");


#
# _after_run()
#
# 1. records the time it took to execute the test.
# 2. calls approximate testers if this is allowed by options.
#

sub _after_run
{
    my $self = shift;

    my $execution_context = shift;

    my $test_result = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    my $time_start = $self->{time_start};

    my $time_elapsed = tv_interval($time_start);

    # fill in the elapsed time in the report

    if ($option_timings)
    {
	my $module_definition = $self->{module_definition};

	my $module_name = $module_definition->{name};

	my $description = $self->{description};

	$global_test_report->{timings}->{$module_name}->{$description} = $time_elapsed;
    }

    # attempt to correct if the result is almost correct

    #! perl code does not produce a full test result object, so we cannot try to correct an almost correct result

    if (not (ref $command eq 'CODE'))
    {
	# if things don't match

	if ($test_result->{error})
	{
	    my $module_definition = $self->{module_definition};

	    # if allowed to compare numerically

	    if ((!$self->{string_only}
		 && $option_numerical_compare)

		# or forced to compare numerically

		|| $self->{numerical_compare}
		|| $command_definition->{numerical_compare}
		|| $module_definition->{numerical_compare})
	    {
		# compare numerically

		push @{$global_test_report->{numerical_compare}}, $self->{description};

		#! this call uses the $read variable, making it harder to bring this call
		#! outside the condition that tests if this variable is defined
		#! this variable contains the expected output

		$test_result->{error}
		    = Neurospaces::Tester::Comparators::numerical
		        (
			 {
			  description => $self->{description},
			 },
			 $test_result->{before_match},
			 $test_result->{expected},
			 $test_result->{error});
	    }
	}
    }

    # return result

    return $test_result;
}


sub after_testing
{
    my $self = shift;

    my $execution_context = shift;

    my $test_result = shift;

    return $self->_after_run($execution_context, $test_result);
}


sub _before_run
{
    my $self = shift;

    my $execution_context = shift;

    my $fd_output = shift;

    # record time

    use Time::HiRes qw(gettimeofday tv_interval);

    my $time_start = [ gettimeofday(), ];

    $self->{time_start} = $time_start;
}


#
# sub before_testing()
#
# Calls ->_before_run() to record the current time.
#
# This method is overriden in the Interactive package.
#

sub before_testing
{
    my $self = shift;

    my $execution_context = shift;

    my $fd_output = shift;

    # record time

    $self->_before_run($execution_context, $fd_output);
}


sub rebless
{
    my $command_test = shift;

    my $target_package = "Heterarch::Test::CommandTest";

    if (ref $command_test->{write} eq 'ARRAY')
    {
	# command_object

	$target_package = "Heterarch::Test::CommandTest::CommandObject";

	my $writes = $command_test->{write};

	foreach my $write (@$writes)
	{
	    my $method = $write->{method};

	    my $arguments = $write->{arguments};
	}

	# compare the read data with what is expected

	if (exists $command_test->{read})
	{
	    my $read = $command_test->{read};
	}
    }

    # if the command_definition has executable perl code

    # elsif (ref $command eq 'CODE')
    # {
	# $target_package = "Heterarch::Test::CommandTest::PerlCode";

    # }

    # else the command definition runs a shell command

    else
    {
	# set read, wait and write strings

	$target_package = "Heterarch::Test::CommandTest::Interactive";

	my $read = $command_test->{read};

	my $wait = $command_test->{wait};

	my $write = $command_test->{write};

	# set timeout, defaults to two seconds, but for newly created processes add two additional seconds.

	my $timeout
	    = (defined $command_test->{timeout}
	       ? $command_test->{timeout} * $option_timeout_multiplier
	       : (0
		  ? 4 * $option_timeout_multiplier
		  : 2 * $option_timeout_multiplier));

	# write

	if (ref $write eq 'HASH')
	{
	}
	elsif (defined $write)
	{
	    # send write to the command under test via the expect object
	}

	# wait

	if ($wait)
	{
	    # wait for $wait seconds
	}

	# check the shell command that will be executed with a timeout

	my $shell = $command_test->{shell};

	if (ref $shell eq 'HASH')
	{
	    #! not sure what this can be
	}
	elsif (defined $shell)
	{
	    # run the command for a maximum of $timeout seconds

	    # fall through
	}

	# if there is an external application to test the final output

	#! the previous shell command was a post-processor, this one is a tester

	my $tester = $command_test->{tester};

	if ($tester)
	{
	    $target_package = "Heterarch::Test::CommandTest::Interactive::ShellTester";

	    # set the external tester that compares the generated output with what is expected

	    #! the external tester runs an executable shell command

	    my $shell = $tester->{shell};

	    # there must be a shell command we should run

	    if ($shell)
	    {
		my $produced = `$shell`;

		my $test_result
		    = {
		       expected => defined $tester->{expected} ? $tester->{expected} : '',
		      };

		if (($produced eq ''
		     and $test_result->{expected} ne '')
		    or ($produced ne ''
			and $test_result->{expected} eq ''))
		{
		    $test_result->{error} = "external tester produces output that is different from what is expected, one is empty, the other is not";

		    $test_result->{before_match} = $produced;
		}
		elsif ($produced =~ /$test_result->{expected}/)
		{
		}
		else
		{
		    $test_result->{error} = "the external tester produces output that is different from the expected output";

		    $test_result->{before_match} = $produced;
		}
	    }
	    else
	    {
		die "$0: *** Error: external testers must have a shell command";
	    }
	}

	# read

	elsif (defined $read)
	{
	    # if literal text expected

	    if (!ref $read)
	    {
		if (exists $command_test->{white_space})
		{
		    if ($command_test->{white_space} eq 'convert seen 0a to 0d 0a newlines')
		    {
			$read =~ s(\x0a)(\x0d\x0a)g;
		    }
		}

		$target_package = "Heterarch::Test::CommandTest::Interactive::Literal";

		# expect $read for output
	    }

	    # if array, means regex match

	    elsif (ref $read eq 'ARRAY')
	    {
		# expect $read to match as a regex with the output

		$target_package = "Heterarch::Test::CommandTest::Interactive::Regex";

	    }

	    # else, hash: one of several alternatives expected

	    else
	    {
		# compose the alternatives regex

		if ($read->{alternatives})
		{
		    # expect that the output literally matches with one of the given alternatives

		    $target_package = "Heterarch::Test::CommandTest::Interactive::Alternatives";

		    my $alternatives = $read->{alternatives};
		}

		# if an application output file is expected

		elsif ($read->{application_output_file})
		{
		    $target_package = "Heterarch::Test::CommandTest::Interactive::File";

		    # read it

		    my $application_output_file = $read->{application_output_file};

		    # and literally compare it with an expected output file or with the {expected_output}

		    my $expected_output_file = $read->{expected_output_file};

		    local $/;

		    my $application_output = `cat "$application_output_file"`;

		    if ($expected_output_file)
		    {
		    }
		    elsif (defined $read->{expected_output})
		    {
		    }
		}

		# if there is a shell command given

		elsif ($read->{shell})
		{
		    $target_package = "Heterarch::Test::CommandTest::Interactive::Shell";

		    # run it and capture its output

		    my $shell = $read->{shell};

		    # and compare its output with the output of the application
		}
		else
		{
		    my $description = $command_test->{description};

		    die "test not understood by $0, aborting (illegal read clause for $description)";
		}
	    }
	}
    }

    # rebless

    bless $command_test, $target_package;

    # return result

    return $command_test;
}


#
# run()
#
# The default implementation executes perl code in the command definition.
#

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # expect this output

    my $test_result;

    # if the command_definition has executable perl code

    if (ref $command eq 'CODE')
    {
	# execute the perl code

	use Cwd;

	my $directory = getcwd();

	my $error
	    = &$command
	    (
	     $self,
	     {
	      c_code => {
			 directory => $directory,
			},
	     },
	    );

	$test_result
	    = {
	       error => $error,
	      };
    }

    # return result

    return $test_result;
}


package Heterarch::Test::CommandTest::CommandObject;


our @ISA = ("Heterarch::Test::CommandTest");


#
# run()
#
# write the write clauses of the command test to the test object.
# expect the contents of the read clause for output.
#

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # expect this output

    my $test_result;

    # record time

    $self->before_testing($execution_context, $fd_output);

    # if the command_definition instantiated an object

    my $command_object = $executor->{command_object};

    # write data to the object using the given methods
    # and arguments and obtain a result from the
    # object

    my $result;

    my $last_method;

    my $writes = $self->{write};

    foreach my $write (@$writes)
    {
	my $method = $write->{method};

	$last_method = $method;

	my $arguments = $write->{arguments};

	$result = $command_object->$method(@$arguments);
    }

    # compare the read data with what is expected

    if (exists $self->{read})
    {
	my $read = $self->{read};

	$test_result
	    = {
	       error => ($result eq $read
			 ? ""
			 : "$last_method returned $result, expected $read"),
	       before_match => $result,
	       expected => $read,
	      };
    }

    # try correcting

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::CommandTest::Interactive;

#
# The interactive package provides methods for testing interactive
# commands:
#
# - waiting for predetermined amount of time.
#
# - post processing of application results before the test takes
#   place.
#
# - record execution times.
#
# It overrides the ->before_testing() method to conveniently call some
# of these methods.
#
# Derived packages should only override the ->run() method.
#


our @ISA = ("Heterarch::Test::CommandTest");


sub before_testing
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    # record time

    $self->_before_run($execution_context, $fd_output);

    # do write processing

    $self->_process_write($executor, $fd_output);

    # do wait processing

    $self->_process_wait($execution_context, $fd_output);

    # do post processing

    $self->_prepare_run($execution_context, $fd_output);
}


# compute a sensible timeout for interaction with the command that is
# being tested.

sub get_timeout
{
    my $self = shift;

    my $test_startup = $self->{test_startup};

    my $result
	= (defined $self->{timeout}
	   ? $self->{timeout} * $option_timeout_multiplier
	   : ($test_startup
	      ? 4 * $option_timeout_multiplier
	      : 2 * $option_timeout_multiplier));

    return $result;
}


#
# _prepare_run()
#
# Provides processing of a 'shell' clause to post-process results
# generated by the application.
#

sub _prepare_run
{
    my $self = shift;

    my $execution_context = shift;

    my $fd_output = shift;

    # check the shell command that will be executed with a timeout

    my $shell = $self->{shell};

    if (ref $shell eq 'HASH')
    {
	#! not sure what this can be
    }
    elsif (defined $shell)
    {
	# run the command for a maximum of $timeout seconds

	# allow it to post-process results generated by the command that is being tested

	my $timeout = $self->get_timeout();

	print $fd_output "*** Shell: timeout $timeout $shell\n";

	system "timeout $timeout $shell";

	#t check the output or status of the shell command?

	# fall through
    }

    #t return something sensible

    return $?;
}


sub _process_wait
{
    my $self = shift;

    my $execution_context = shift;

    my $fd_output = shift;

    my $wait = $self->{wait};

    # wait

    if ($wait)
    {
	print $fd_output "*** Wait: $wait\n";

	select(undef, undef, undef, $wait);
    }

}


sub _process_write
{
    my $self = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $write = $self->{write};

    # write

    if (ref $write eq 'HASH')
    {
    }
    elsif (defined $write)
    {
	print $fd_output "*** Write: $write\n";

	$executor->{expect_object}->send("$write\n");
    }
}


# I believe this is only a template for derived classes.

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # record time, do write, wait processing, pre-processing

    $self->before_testing($execution_context, $executor, $fd_output);

    # try correcting

    my $test_result;

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::CommandTest::Interactive::ShellTester;


our @ISA = ("Heterarch::Test::CommandTest::Interactive");


# write the write clauses of the command test.
# use the shell command in the tester clause to produce the application output.
#   the shell command should find the produced output and process it.
# compare this output with the expected output.

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # record time, do write, wait processing, pre-processing

    $self->before_testing($execution_context, $executor, $fd_output);

    # if there is an external application to test the final output

    #! the previous shell command was a post-processor, this one is a tester

    my $tester = $self->{tester};

    # set the external tester that compares the generated output with what is expected

    #! the external tester runs an executable shell command

    my $shell = $tester->{shell};

    my $produced = `$shell`;

    my $test_result
	= {
	   expected => defined $tester->{expected} ? $tester->{expected} : '',
	  };

    #			    if ($expected)
    #				$expected = quotemeta $tester->{expected};

    if (($produced eq ''
	 and $test_result->{expected} ne '')
	or ($produced ne ''
	    and $test_result->{expected} eq ''))
    {
	$test_result->{error}
	    = "external tester produces output that is different from what is expected, one is empty, the other is not";

	$test_result->{before_match} = $produced;
    }
    elsif ($produced =~ /$test_result->{expected}/)
    {
    }
    else
    {
	$test_result->{error} = "the external tester produces output that is different from the expected output";

	$test_result->{before_match} = $produced;
    }

    # try correcting

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::CommandTest::Interactive::Literal;


our @ISA = ("Heterarch::Test::CommandTest::Interactive");


# write the write clauses of the command test.
# read the application output using the expect object.
# literally compare the application output with the expected output.

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # record time, do write, wait processing, pre-processing

    $self->before_testing($execution_context, $executor, $fd_output);

    # set the expected output

    my $read = $self->{read};

    # if literal text expected

    if (exists $self->{white_space})
    {
	if ($self->{white_space} eq 'convert seen 0a to 0d 0a newlines')
	{
	    print $fd_output "*** Converting seen 0a to 0d 0a newlines\n";

	    $read =~ s(\x0a)(\x0d\x0a)g;
	}
    }

    my $timeout = $self->get_timeout();

    my ($matched_pattern_position,
	$error,
	$successfully_matching_string,
	$before_match,
	$after_match)
	= $executor->{expect_object}->expect($timeout, $read, );

    my $test_result
	= {
	   matched_pattern_position => $matched_pattern_position,
	   error => $error,
	   successfully_matching_string => $successfully_matching_string,
	   before_match => $before_match,
	   after_match => $after_match,
	   expected => $read,
	  };

    # try correcting

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::CommandTest::Interactive::Regex;


our @ISA = ("Heterarch::Test::CommandTest::Interactive");


# write the write clauses of the command test.
# read the application output using the expect object.
# compare the application output with the expected output as a regex.

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # record time, do write, wait processing, pre-processing

    $self->before_testing($execution_context, $executor, $fd_output);

    # set the expected output

    my $read = $self->{read};

    # if array, means regex match

    my $timeout = $self->get_timeout();

    my ($matched_pattern_position,
	$error,
	$successfully_matching_string,
	$before_match,
	$after_match)
	= $executor->{expect_object}->expect($timeout, @$read, );

    my $test_result
	= {
	   matched_pattern_position => $matched_pattern_position,
	   error => $error,
	   successfully_matching_string => $successfully_matching_string,
	   before_match => $before_match,
	   after_match => $after_match,

	   #! skip the expect '-re' flag

	   expected => $read->[1],
	  };

    # try correcting

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::CommandTest::Interactive::Alternatives;


our @ISA = ("Heterarch::Test::CommandTest::Interactive");


# write the write clauses of the command test.
# read the application output using the expect object.
# compare the application output with the expected output which is a list of alternatives.

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # record time, do write, wait processing, pre-processing

    $self->before_testing($execution_context, $executor, $fd_output);

    # compose the alternatives regex

    my $read = $self->{read};

    my $alternatives = $read->{alternatives};

    my $test_result
	= {
	   expected => '(' . (join '|', map { quotemeta } @$alternatives) . ')',
	  };

    # read from the application

    my $timeout = $self->get_timeout();

    my ($matched_pattern_position,
	$error,
	$successfully_matching_string,
	$before_match,
	$after_match)
	= $executor->{expect_object}->expect($timeout, "-re", $test_result->{expected}, );

    $test_result->{matched_pattern_position} = $matched_pattern_position;
    $test_result->{error} = $error;
    $test_result->{successfully_matching_string} = $successfully_matching_string;
    $test_result->{before_match} = $before_match;
    $test_result->{after_match} = $after_match;

    # try correcting

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::CommandTest::Interactive::File;


our @ISA = ("Heterarch::Test::CommandTest::Interactive");


# write the write clauses of the command test.
# read the application output from a file.
# compare the application output with the expected output which is a file.

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # record time, do write, wait processing, pre-processing

    $self->before_testing($execution_context, $executor, $fd_output);

    # read the application output file

    my $read = $self->{read};

    my $application_output_file = $read->{application_output_file};

    my $pwd = `pwd`;

    chomp $pwd;

    print $fd_output "*** Application output file: $application_output_file (in $pwd)\n";

    # and compare it with the expected output file

    my $expected_output_file = $read->{expected_output_file};

    local $/;

    my $application_output = `cat "$application_output_file"`;

    my $expected_output;

    if ($expected_output_file)
    {
	print $fd_output "*** Expected output file: $expected_output_file\n";

	$expected_output = `cat "$expected_output_file"`;
    }
    elsif (defined $read->{expected_output})
    {
	$expected_output = $read->{expected_output};
    }

    my $test_result
	= {
	   expected => $expected_output,
	  };

    if ($expected_output eq $application_output)
    {
	$test_result->{before_match} = $application_output;
    }
    else
    {
	$test_result->{before_match} = $application_output;

	$read = $expected_output;

	$test_result->{error} = 'expected_output does not match application_output';
    }

    # try correcting

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::CommandTest::Interactive::Shell;


our @ISA = ("Heterarch::Test::CommandTest::Interactive");


# write the write clauses of the command test.
# run the given shell command and capture its output.
# read the application output using the expect object.
# compare this output with the application output.

sub ct_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $command = $command_definition->{command};

    # record time, do write, wait processing, pre-processing

    $self->before_testing($execution_context, $executor, $fd_output);

    # run the given shell command and capture its output

    my $read = $self->{read};

    my $shell = $read->{shell};

    my $expected = `$shell`;

    # and compare its output with the output of the application

    my $timeout = $self->get_timeout();

    my ($matched_pattern_position,
	$error,
	$successfully_matching_string,
	$before_match,
	$after_match)
	= $executor->{expect_object}->expect($timeout, $expected, );

    my $test_result
	= {
	   matched_pattern_position => $matched_pattern_position,
	   error => $error,
	   successfully_matching_string => $successfully_matching_string,
	   before_match => $before_match,
	   after_match => $after_match,
	   expected => $expected,
	  };

    # try correcting

    $self->after_testing($execution_context, $test_result);

    # return the final test result

    return $test_result;
}


package Heterarch::Test::Element;


our $selected_output_levels;


# sub is_selected() should implement a logical and of the different selection criteria

sub is_selected
{
    my $self = shift;

    my $fd_output = shift;

    return $self->is_selected_by_output_level_options($fd_output);
}


sub is_selected_by_output_level_options
{
    my $self = shift;

    my $fd_output = shift;

    # map the selected output levels to a hash to facilitate easy detection what has been enabled

    if (not defined $selected_output_levels)
    {
	$selected_output_levels
	    = {
	       map
	       {
		   my $output_level = $_;

		   my $result = { $output_level => "from options", };

		   %$result;
	       }
	       @$option_output_levels,
	      };
    }

    # convert the class to its output level

    use Scalar::Util 'blessed';

    my $type = defined blessed $self && $self->isa("Heterarch::Test::Module") ? "module" : "";

    $type ||= defined blessed $self && $self->isa("Heterarch::Test::CommandDefinition") ? "command_definition" : "";

    $type ||= defined blessed $self && $self->isa("Heterarch::Test::CommandTest") ? "command_test" : "";

    # determine whether this output level was selected

    my $selected = $selected_output_levels->{$type};

    # return result

    return $selected;
}


package Heterarch::Test::ExecutionContext;


#
# _prepre_engine() implements the engine for both preparation and
# reparation that are used for test modules and command definitions.
#

sub _prepre_engine
{
    my $executor_element = shift;

    my $description = shift;

    my $previous_result = shift;

    my $result = 'nothing executed';

    # either execute regular perl code

    if (ref $executor_element eq 'CODE')
    {
	$result = &$executor_element($previous_result);
    }

    # ... or instantiate the object that will do the preparation

    elsif (exists $executor_element->{class})
    {
	# instantiate the object

	my $class = $executor_element->{class};

	my $filename = $class . ".pm";

	$filename =~ s(::)(/)g;

	require $filename;

	my $executor_element_object = eval "$class->new()";

	# apply all the methods and collect the results

	$result = [];

	my $applicators = $executor_element->{applicators};

	foreach my $applicator (@$applicators)
	{
	    my $method = $applicator->{method};

	    my $arguments = $applicator->{arguments};

	    my $applicator_result = $executor_element_object->$method($arguments);

	    if (ref $applicator_result eq 'SCALAR')
	    {
		$result = \ "Error: $description failed ($$applicator_result)";

		last;
	    }

	    push @$result, $applicator_result;
	}
    }

    # or execute an array of system shell commands

    elsif (exists $executor_element->{system_commands})
    {
	my $system_commands = $executor_element->{system_commands};

	if (ref $system_commands eq 'ARRAY')
	{
	    foreach my $system_command (@$system_commands)
	    {
		system $system_command;

		if ($? ne 0)
		{
		    $result = \ "Error: $description failed ($system_command)";

		    last;
		}
	    }
	}
    }

    # return result

    return $result;
}


sub _get_current
{
    my $self = shift;

    my $path = $self->{path};

    my $current = $path->[$#$path];

    return $current;
}


#t to be replaced with instance variables

our $command_definition_counter_per_module = 0;
our $command_definition_counter_global = 0;

our $command_test_counter_per_command_definition = 0;
our $command_test_counter_per_module = 0;
our $command_test_counter_global = 0;

our $module_counter_global = 0;


sub _maintain_counters
{
    my $self = shift;

    my $current = shift;

    my $command_definition = $current->[1];

    my $command_test = $current->[2];

    my $module_definition = $current->[0];

    if (defined $command_test)
    {
	$command_test_counter_global++;
	$command_test_counter_per_module++;
	$command_test_counter_per_command_definition++;
    }
    elsif (defined $command_definition)
    {
	$command_definition_counter_global++;
	$command_definition_counter_per_module++;

	$command_test_counter_per_command_definition = 0;
    }
    elsif (defined $module_definition)
    {
	$module_counter_global++;

	$command_definition_counter_per_module = 0;
	$command_test_counter_per_module = 0;
    }
    else
    {
	die "$0: internal error when _maintain_counters(): nothing left to count\n";
    }
}


sub _push_current
{
    my $self = shift;

    my $current = shift;

    my $path = $self->{path};

    push @$path, $current;

    $self->_maintain_counters($current);

}


sub _pop_current
{
    my $self = shift;

    my $path = $self->{path};

    my $result = pop @$path;

    return $result;
}


sub _push_preparation_result
{
    my $self = shift;

    my $preparation_result = shift;

    my $path = $self->{preparation_result};

    push @$path, $preparation_result;

}


sub _pop_preparation_result
{
    my $self = shift;

    my $path = $self->{preparation_result};

    my $result = pop @$path;

    return $result;
}


sub ec_end
{
    my $self = shift;

    my $module_definition = shift;

    my $command_definition = shift;

    my $command_test = shift;

    #! note: reparation_error has already been processed

    my $reparation_error = $self->_reparation();

    if ($global_test_report->{output_format} eq 'executor')
    {
	my $fd_output = $self->{fd_output};

	my $module_name = $module_definition->{name};

	my $description;

	my $name;

	if (defined $command_test)
	{
	    $description = "*** Test: $command_test->{description}";

	    $name = "";
	}
	elsif (defined $command_definition)
	{
	    $description = "command $command_definition->{description}";

	    $name = " ($command_definition->{command})";
	}
	else
	{
	    $description = "module $module_definition->{description}";

	    $name = " ($module_definition->{name})";
	}

	if (defined $command_test)
	{
	    # no output here, the test output is emitted in ->ec_examine()
	}
	else
	{
	    Heterarch::Test::Reporting::report_message_end($fd_output, "End for tests of $description
Total of $global_test_report->{global}->{test_counters}->{command_test} test(s) (encountered $global_error_count error(s) so far)");
	}
    }

    $self->_pop_current();
}


sub get_command_definition
{
    my $self = shift;

    my $current = $self->_get_current();

    my $command_definition = $current->[1];

    return $command_definition;
}


sub get_command_definition_counter_per_module
{
    my $self = shift;

    #t to be replaced with instance variables

    return $command_definition_counter_per_module;
}


sub get_command_test
{
    my $self = shift;

    my $current = $self->_get_current();

    my $command_test = $current->[2];

    return $command_test;
}


sub get_command_test_counter_per_command_definition
{
    my $self = shift;

    #t to be replaced with instance variables

    return $command_test_counter_per_command_definition;
}


sub get_module_definition
{
    my $self = shift;

    my $current = $self->_get_current();

    my $module_definition = $current->[0];

    return $module_definition;
}


sub get_module_definition_counter
{
    my $self = shift;

    #t to be replaced with instance variables

    return $module_counter_global;
}


sub has_error_flag
{
    my $self = shift;

    my $module_definition = shift;

    my $command_definition = shift;

    my $command_test = shift;

    my $fd_output = $self->{fd_output};

    my $module_name = $module_definition->{name};

    my $active_element
	= (defined $command_test
	   ? $command_test
	   : (defined $command_definition
	      ? $command_definition
	      : $module_definition));

    my $error = $active_element->{error};

    my $description = $active_element->{description};

    if ($error)
    {
	Heterarch::Test::Reporting::report_error_add
		(
		 $fd_output,
		 {
		  description => $description,
		  error => "this test was tagged with the error flag",
		  module_name => $module_name,
		  subdescription => $error,
		 },
		);

	return 1;
    }
    else
    {
	return 0;
    }
}


sub is_disabled
{
    my $self = shift;

    my $module_definition = shift;

    my $command_definition = shift;

    my $command_test = shift;

    my $fd_output = $self->{fd_output};

    my $module_name = $module_definition->{name};

    my $active_element
	= (defined $command_test
	   ? $command_test
	   : (defined $command_definition
	      ? $command_definition
	      : $module_definition));

    my $disabled = $active_element->{disabled};

    if ($disabled)
    {
	my $description = $active_element->{description};

	my $report_key
	    = (defined $command_test
	       ? "command_tests"
	       : (defined $command_definition
		  ? "command_definitions"
		  : "modules"));

	$global_test_report->{disabled}->{$report_key}->{$module_name}->{$description} = $disabled;

	if ($global_test_report->{output_format} eq 'executor')
	{
	    Heterarch::Test::Reporting::report_message_info($fd_output, "Tests of $description are disabled ($disabled)
Total of $global_test_report->{global}->{test_counters}->{command_test} test(s) (encountered $global_error_count error(s) so far)");
	}

	return 1;
    }
    else
    {
	return 0;
    }
}


sub is_selected
{
    my $self = shift;

    my $module_definition = shift;

    my $command_definition = shift;

    my $command_test = shift;

    my $fd_output = $self->{fd_output};

    my $module_name = $module_definition->{name};

    my $active_element
	= (defined $command_test
	   ? $command_test
	   : (defined $command_definition
	      ? $command_definition
	      : $module_definition));

    my $result = $active_element->is_selected($fd_output);

    return $result;
}


sub _preparation
{
    my $self = shift;

    my $current = $self->_get_current();

    my $module_definition = $current->[0];

    my $command_definition = $current->[1];

    my $command_test = $current->[2];

    # command_tests with a harnessing clause are unsupported

    if ($command_test)
    {
	return undef;
    }

    my $fd_output = $self->{fd_output};

    my $module_name = $module_definition->{name};

    if ($command_definition)
    {
	if (exists $command_definition->{preparation}
	    and exists $command_definition->{harnessing})
	{
	    die "$0: command_definition ($command_definition->{description}) has both a preparation and harnessing clause\n";
	}
    }
    else
    {
	if (exists $module_definition->{preparation}
	    and exists $module_definition->{harnessing})
	{
	    die "$0: module_definition ($module_definition->{description}) has both a preparation and harnessing clause\n";
	}
    }

    my $active_element
	= (defined $command_test
	   ? $command_test
	   : (defined $command_definition
	      ? $command_definition
	      : $module_definition));

    # convert the old format preparation / reparation to the new format of harnessing

    if (not exists $active_element->{harnessing})
    {
	if (exists $active_element->{preparation})
	{
	    $active_element->{harnessing}->{preparation} = $active_element->{preparation};

	    delete $active_element->{preparation};
	}

	if (exists $active_element->{reparation})
	{
	    $active_element->{harnessing}->{reparation} = $active_element->{reparation};

	    delete $active_element->{reparation};
	}
    }

    # process the harnessing preparation clause

    my $harnessing = $active_element->{harnessing};

    my $preparation = $harnessing->{preparation};

    my $preparation_result;

    # if we are really running tests and there is a preparation clause

    if (($global_test_report->{output_format} eq 'executor')
	and $preparation)
    {
	# give diagnostics

	my $preparation_name
	    = ($command_definition
	       ? "command '$command_definition->{command}'"
	       : "module '$module_definition->{name}'");

	Heterarch::Test::Reporting::report_message($fd_output, "*** Preparing $preparation_name ($preparation->{description})");

	# execute the preparation clause

	my $preparer = $preparation->{preparer};

	$preparation_result = _prepre_engine($preparer, 'preparation');

	# if the preparer returned an error string

	my $preparation_error;

	if (ref $preparation_result eq 'SCALAR')
	{
	    # obtain the error string

	    $preparation_error = $$preparation_result;
	}

	# process errors

	if ($preparation_error)
	{
	    my $module_name = $module_definition->{name};

	    my $subdescription
		= ($command_definition
		   ? $command_definition->{description}
		   : $module_definition->{description});

	    Heterarch::Test::Reporting::report_error_add
		(
		 $fd_output,
		 {
		     description => $preparation_name,
		     error => $preparation_error,
		     module_name => $module_name,
		     subdescription => $subdescription,
		 },
		);
	}
    }

    # return result

    return $preparation_result;
}


sub new
{
    my $package = shift;

    my $fd_output = shift;

    my $options = shift || {};

    my $self
	= {
	   %$options,
	   fd_output => $fd_output,
	   path => [],
	   preparation_result => [],
	  };

    bless $self, $package;

    return $self;
}


sub _reparation
{
    my $self = shift;

    my $current = $self->_get_current();

    my $module_definition = $current->[0];

    my $command_definition = $current->[1];

    my $command_test = $current->[2];

    my $preparation_result = $self->_pop_preparation_result();

    # command_tests with a harnessing clause are unsupported

    if ($command_test)
    {
	return undef;
    }

    my $active_element
	= (defined $command_test
	   ? $command_test
	   : (defined $command_definition
	      ? $command_definition
	      : $module_definition));

    my $module_name = $module_definition->{name};

    # process the harnessing reparation clause

    my $harnessing
	= ($command_definition
	   ? $command_definition->{harnessing}
	   : $module_definition->{harnessing});

    my $reparation = $harnessing->{reparation};

    my $reparation_name
        = ($command_definition
	    ? ($command_definition->{command}
		? "command '$command_definition->{command}'"
		: "module '$module_definition->{name}'")
	    : ($command_definition->{recycle}
		? "these tests use the previous running command ($command_definition->{recycle})"
		: "*** Warning: this command definition is incomplete (no command and no recycle clause)")
	);

    my $reparation_error;

    # if there is a reparation clause

    if (($global_test_report->{output_format} eq 'executor')
	and $reparation)
    {
	# give diagnostics

	my $fd_output = $self->{fd_output};

	Heterarch::Test::Reporting::report_message($fd_output, "*** Reparing $reparation_name ($reparation->{description})");

	# execute the reparation clause

	my $reparer = $reparation->{reparer};

	$reparation_error = _prepre_engine($reparer, 'reparation', $preparation_result);

	# process errors

	if ($reparation_error)
	{
	    my $module_name = $module_definition->{name};

	    my $subdescription
		= ($command_definition
		   ? $command_definition->{description}
		   : $module_definition->{description});

	    Heterarch::Test::Reporting::report_error_add
		(
		 $fd_output,
		 {
		     description => $reparation_name,
		     error => $reparation_error,
		     module_name => $module_name,
		     subdescription => $subdescription,
		 },
		);
	}
    }

    return $reparation_error;
}


#
# sub start
#
# Apply selection criteria to the current context.
# Prepare the test environment.
#

sub ec_examine
{
    my $self = shift;

    my $module_definition = shift;

    my $command_definition = shift;

    my $command_test = shift;

    # set default result: everything ok

    my $result;

    # if this element is selected by active options

    if (not $self->is_selected($module_definition, $command_definition, $command_test))
    {
	return "not selected";
    }

    # if this test is disabled

    if ($self->is_disabled($module_definition, $command_definition, $command_test))
    {
	# return to skip it

	return "is disbled";
    }

    # if this test has been flagged with an error

    if ($self->has_error_flag($module_definition, $command_definition, $command_test))
    {
	# return to skip it

	return "has the error flag";;
    }

    # update the path of execution

    $self->_push_current( [ $module_definition, $command_definition, $command_test, ] );

    # give diagnostics

    if ($global_test_report->{output_format} eq 'executor')
    {
	my $fd_output = $self->{fd_output};

	my $comment;

	my $description;

	my $name;

	if (defined $command_test)
	{
	    $comment = $command_test->{comment};

	    $description = "*** Test: $command_test->{description}";

	    $name = "";
	}
	elsif (defined $command_definition)
	{
	    $comment = $command_definition->{comment};

	    $description = "command $command_definition->{description}";

	    $name = " ($command_definition->{command})";
	}
	else
	{
	    $comment = $module_definition->{comment};

	    $description = "module $module_definition->{description}";

	    $name = " ($module_definition->{name})";
	}

	if (defined $command_test)
	{
	    Heterarch::Test::Reporting::report_message($fd_output, "$description");
	}
	else
	{
	    Heterarch::Test::Reporting::report_message_start($fd_output, "Running tests of $description$name");
	}

	if ($comment)
	{
	    Heterarch::Test::Reporting::report_message($fd_output, "*** Comment: $comment");
	}
    }

    # prepare the test environment

    my $preparation_result = $self->_preparation();

    $self->_push_preparation_result($preparation_result);

    # return result: should be undef here

    return $result;
}


sub terminate
{
    my $self = shift;

    my $fd_output = $self->{fd_output};

    my $path = $self->{path};

    if (scalar @$path)
    {
	my $module_definition = $self->get_module_definition();

	my $description = "module $module_definition->{description}";

	my $module_name = $module_definition->{name};

	Heterarch::Test::Reporting::report_error_add
	    (
	     $fd_output,
	     {
	      description => "execution context invalid",
	      error => "left over elements in the execution context after it was terminated",
	      module_name => $module_name,
	     },
	    );
    }
}


package Heterarch::Test::Executor;

#
# This is the default Heterarch::Test::Executor.  It provides a link
# with the execution context and implements ->ex_mo_prepare(),
# ex_cd_start() and ex_ct_run() methods that call the appropriate
# module, command_definition and command_test methods for implementing
# test execution.
#


sub new
{
    my $package = shift;

    my $execution_context = shift;

    my $fd_output = $execution_context->{fd_output};

    my $options = shift || {};

    my $self
	= {
	   %$options,
	   execution_context => $execution_context,
	   fd_output => $fd_output,
	  };

    bless $self, $package;

    return $self;
}


#
# Create the infrastructure for keeping module test results.
#
# For a base executor, this is currently a no-op.
#
# There is currently a global infrastructure for storing real test
# results, rather than a local one.
#

sub ex_mo_prepare
{
    my $self = shift;

}


#
# ->run() is called to execute one command test.
#
# this executor trivially delegates this call to the command_test object
#
# A derived class can inspect the elements found in the test
# specification and, for instance, print them to a file.
#

sub ex_ct_run
{
    my $self = shift;

    my $command_test = shift;

    my $execution_context = $self->{execution_context};

    my $fd_output = $self->{fd_output};

    return $command_test->ct_run($execution_context, $self, $fd_output);
}


#
# ->start() is called to start the tests of a single command definition.
#
# It instantiate the tester:
#
# 1. an object given its class.
# 2. a piece of perl code that is run.
# 3. an interactive system shell command.
#

sub ex_cd_start
{
    my $self = shift;

    my $execution_context = $self->{execution_context};

    my $module_definition = $execution_context->get_module_definition();

    my $command_definition = $execution_context->get_command_definition();

    my $fd_output = $self->{fd_output};

    $command_definition->cd_start($self, $fd_output);
}


package Heterarch::Test::Library;


sub construct
{
    # find the test specifications

    my $test_modules
	= [
	   map
	   {
	       chomp; $_;
	   }
	   sort
	   `find $global_config->{tests_directory} -name "*.t"`,
	  ];

    return $test_modules;
}


sub expand
{
    my $fd_output = shift;;

    my $library = shift;

    my $test_modules = [];

    # read the introduction

    my $html_introduction;

    {
	local $/; # enable 'slurp' mode

	my $file = IO::File->new("<$global_config->{introduction_filename}");

	$html_introduction = <$file>;

	$file->close();
    }

    # parse all modules

    foreach my $test_module (@$library)
    {
	if ($test_module !~ /$option_regex_selector/i)
	{
	    next;
	}

	my $module_definition = readfile($fd_output, $test_module);

	if ($module_definition)
	{
	    push @$test_modules, $module_definition;
	}
    }

    if ($option_flattenout)
    {
	# sort modules

	#t first need to transform: select command_definitions, flatten
	#t out, keep the module names (for referencing errors).

	my $transformator
	    = Data::Transformator->new
		(
		 apply_identity_transformation => 0,
		 name => 'test-module-selector',
		 contents => $test_modules,
		 separator => '`',
		 array_filter =>
		 sub
		 {
		     my ($context, $component) = @_;

		     # never filter for the first two component in the path

		     my $depth = $context->{array};
		     $depth = $#$depth;

		     if ($depth < 2)
		     {
			 return 1;
		     }

		     # extract the data: command definitions with test commands

		     $context->{path} =~ m|^[^/]*/([^/]*)/([^/]*)|;

		     my $content = Data::Transformator::_context_get_current_content($context);

		     # push it onto the result

		     my $result = Data::Transformator::_context_get_main_result($context);

		     if (!$result->{content})
		     {
			 $result->{content} = [];
		     }

		     push @{$result->{content}}, $content;

		     # add the module name

		     my $module_name = $context->{array}->[1]->{content}->{name};

		     $content->{module_name} = $module_name;

		     # add the module description

		     my $module_description = $context->{array}->[1]->{content}->{description};

		     $content->{module_description} = $module_description;

		     # result is known, everything gets filtered

		     0;
		 },
		);

	#t for an empty array as content, the transformator returns an
	#t undef, this is a bug that still needs fixing.

	my $tests = $transformator->transform() || [];

	# sort the flattened test definitions

	$tests
	    = [
	       sort
	       {
		   my $module1 = $a;
		   my $module2 = $b;

		   my $command1 = $module1->{command};
		   my $command2 = $module2->{command};

		   my $command1_arguments = $module1->{arguments} || [];
		   my $command2_arguments = $module2->{arguments} || [];

		   my $command1_string = join ' ', $command1, @$command1_arguments;
		   my $command2_string = join ' ', $command2, @$command2_arguments;

		   my $comparison = $command1_string cmp $command2_string;

		   if ($module1->{tester_head})
		   {
		       $comparison = -1;
		   }
		   elsif ($module2->{tester_head})
		   {
		       $comparison = 1;
		   }

		   $comparison;
	       }
	       @$tests,
	      ];

	# transform back to the regular test module format by putting
	# every command definition in its own module

	$tests
	    = [
	       map
	       {
		   my $command_definition = $_;

		   (
		    {
		     command_definitions => [
					     $command_definition,
					    ],
		     description => $command_definition->{module_description},
		     name => $command_definition->{module_name},
		    }
		   );
	       }
	       @$tests,
	      ];

	# set result

	$test_modules = $tests;
    }

    # set result

    my $result
	= {
	   html_introduction => $html_introduction,
	   test_modules => $test_modules,
	  };

    # return result

    return $result;
}


sub is_json
{
    my $filename = shift;

    my $result;

    use IO::File;

    my $fh = IO::File->new();

    if ($fh->open("< $filename"))
    {
	my $first_line = <$fh>;

	$first_line =~ s/(\s)*//g;

	if ($first_line =~ m'^\{')
	{
	    $result = 'is_json';
	}

	$fh->close;
    }

    return $result;
}


sub is_yaml
{
    my $filename = shift;

    my $result;

    use IO::File;

    my $fh = IO::File->new();

    if ($fh->open("< $filename"))
    {
	my $first_line = <$fh>;

	$first_line =~ s/(\s)*//g;

	if ($first_line eq '---')
	{
	    $result = 'is_yaml';
	}

	$fh->close;
    }

    return $result;
}


sub randomize_order
{
    my $test_modules = shift;

    my $random_seed = shift;

    # by default the randomizer will choose a random_seed

    if (not defined $random_seed)
    {
	$random_seed = time() ^ $$ ^ unpack "%L*", `ps axww | gzip`;
    }

    # set the random_seed

    srand($random_seed);

    # and shuffle the modules

    require List::Util;

    return ( [ List::Util::shuffle(@$test_modules), ], $random_seed );
}


sub readfile
{
    my $fd_output = shift;

    my $pathname = shift;

    my $result;

    if (is_yaml($pathname))
    {
	use YAML;

	$result = YAML::LoadFile($pathname);

	if ($@)
	{
	    report_error_add
	    (
	     $fd_output,
	     {
	      description => $@,
	      error => $@,
	      module_name => $pathname,
	     },
	    );
	}
    }
    elsif (is_json($pathname))
    {
	local $/; # enable 'slurp' mode

	my $file = IO::File->new("<$pathname");

	my $json_text = <$file>;

	$file->close();

	use JSON;

	my $json = JSON->new();

	#! allow code refs, they are converted to nulls

	$json->allow_unknown(1);

	$result = $json->decode($json_text);

	if ($@)
	{
	    report_error_add
	    (
	     $fd_output,
	     {
	      description => $@,
	      error => $@,
	      module_name => $pathname,
	     },
	    );
	}
    }
    else
    {
	$result = do $pathname;

	if ($@)
	{
	    report_error_add
	    (
	     $fd_output,
	     {
	      description => $@,
	      error => $@,
	      module_name => $pathname,
	     },
	    );
	}
    }

    $pathname =~ m((.*)/(.*));

    my $filename = $2;

    if ($option_dump_json)
    {
	use JSON;

	my $json = JSON->new();

	#! allow code refs, they are converted to nulls

	$json->allow_unknown(1);

	my $file = IO::File->new(">/tmp/$filename.js");

	print $file $json->encode($result);

	$file->close();
    }

    if ($option_dump_perl)
    {
	use Data::Dumper;

	my $file = IO::File->new(">/tmp/$filename.pl");

	print $file Dumper($result);

	$file->close();
    }

    if ($option_dump_yaml)
    {
	YAML::DumpFile("/tmp/$filename.yml", $result);
    }

    # check if the name of the test and the pathname match

    if ($option_check_test_names)
    {
	if ($pathname ne "$global_config->{tests_directory}/$result->{name}")
	{
	    Heterarch::Test::Reporting::report_error_add
		(
		 $fd_output,
		 {
		     description => "the pathname ($pathname) and the test name ($result->{name}) are different",
		     error => "the pathname ($pathname) and the test name ($result->{name}) are different",
		     module_name => $pathname,
		 },
		);
	}
    }

    return $result;
}


package Heterarch::Test::Module;


our @ISA = ("Heterarch::Test::Element");


# could also use Array::Util, but it is not always installed

sub array_union_intersection_difference
{
    my $array1 = shift;

    my $array2 = shift;

    my (@union, @intersection, @difference);

    my %count = ();

    foreach my $element (@$array1, @$array2)
    {
	$count{$element}++
    }

    foreach my $element (keys %count)
    {
	push @union, $element;

	push @{ $count{$element} > 1
		    ? \@intersection
		    : \@difference },
		    $element;
    }

    return ( \@union, \@intersection, \@difference );
}


sub check_description
{
    my $module_definition = shift;

    my $fd_output = shift;

    my $module_name = $module_definition->{name};

    if (!defined $module_definition->{description})
    {
	$module_definition->{description} = $module_definition->{name};

	Heterarch::Test::Reporting::report_error_add
		(
		 $fd_output,
		 {
		  description => 'no module description',
		  error => 'no module description',
		  module_name => $module_name,
		 },
		);
    }

    return $module_definition->{description};
}


sub check_name
{
    my $module_definition = shift;

    my $fd_output = shift;

    if (!defined $module_definition->{name})
    {
	$module_definition->{name} = 'unnamed';

	Heterarch::Test::Reporting::report_error_add
		(
		 $fd_output,
		 {
		  description => 'unnamed module',
		  error => 'unnamed module',
		  module_name => 'unnamed module',
		 },
		);
    }

    return $module_definition->{name};
}


sub is_selected
{
    my $self = shift;

    my $fd_output = shift;

    # implement a logical and of the different selection criteria

    my $result = $self->SUPER::is_selected($fd_output);

    if ($result)
    {
	$result = $self->is_selected_by_tag($fd_output);
    }

    return $result;
}


# return true if this module definition has a tag that matches the tag options.

sub is_selected_by_tag
{
    my $module_definition = shift;

    my $fd_output = shift;

    my $module_name = $module_definition->{name};

    if (not scalar @$option_tags)
    {
	return ":all";
    }

    my $module_definition_tags = $module_definition->{tags};

    # use Array::Util qw(intersect);

    my ( $union, $intersection, $difference ) = array_union_intersection_difference($module_definition_tags, $option_tags);

    if (scalar @$intersection)
    {
	return $intersection;
    }
    else
    {
	if ($global_test_report->{output_format} eq 'executor')
	{
	    Heterarch::Test::Reporting::report_message_info($fd_output, "Module $module_definition->{description} is not selected by tag (" . (join ", ", @$option_tags ) . " are selected tags).
Total of $global_test_report->{global}->{test_counters}->{command_test} test(s) (encountered $global_error_count error(s) so far)");
	}

	$global_test_report->{not_selected_by_tag}->{modules}->{$module_name} = 'not_selected_by_tag';

	return undef;
    }
}


# check if this module is ready to run its tests.
# execute all the command definitions.
# execute all the command tests in these command definitions.

sub md_run
{
    my $self = shift;

    my $execution_context = shift;

    my $executor = shift;

    my $fd_output = shift;

    my $module_name = $self->check_name($fd_output);

    my $module_description = $self->check_description($fd_output);

    # do selection and prepare the test environment

    my $not_started = $execution_context->ec_examine($self);

    if ($not_started)
    {
	return "module start failed ($not_started)";
    }

    # create the infrastructure for keeping module test results

    $executor->ex_mo_prepare();

    # loop over commands for this module

    my $command_definitions = $self->{command_definitions};

    foreach my $command_definition (@$command_definitions)
    {
	bless $command_definition, "Heterarch::Test::CommandDefinition";

	# lookup the class for this command definition

	$command_definition->rebless();

	# do selection and prepare the test environment

	my $not_started = $execution_context->ec_examine($self, $command_definition);

	if ($not_started)
	{
	    next;
	}

	# call the perl code, spaw_new

	# 1. an object given its class.
	# 2. a piece of perl code that is run.
	# 3. an interactive system shell command.

	$executor->ex_cd_start();

	# loop over all tests for this command

	my $command_tests = $command_definition->{command_tests};

	foreach my $command_test (@$command_tests)
	{
	    bless $command_test, "Heterarch::Test::CommandTest";

	    # lookup the class for this command test

	    $command_test->rebless();

	    # emit comment if any

	    my $not_started = $execution_context->ec_examine($self, $command_definition, $command_test);

	    if ($not_started)
	    {
		next;
	    }

	    # run the text executor which will run the command_test

	    my $execution_result = $executor->ex_ct_run($command_test);

	    # process errors

	    my $error = $execution_result->{error};

	    if ($error)
	    {
		my $description = $command_test->{description};

		my $command_definition_description = $command_definition->{description};

		my $message;

		my $before_match = $execution_result->{before_match};

		if ($option_verbose)
		{
		    $message = $before_match;
		}

		Heterarch::Test::Reporting::report_error_add
			(
			 $fd_output,
			 {
			  description => $description,
			  error => $error,
			  expected => $execution_result->{expected},
			  message => $message,
			  module_name => $module_name,
			  seen => $before_match,
			  subdescription => $command_definition_description,
			 },
			);
	    }

	    # end the execution of this test

	    $execution_context->ec_end($self, $command_definition, $command_test);

	    # register if this command had side effects

	    $global_previous_command_side_effects ||= $command_test->{side_effects} || 0;

	    # increment command test count

	    $global_test_report->{global}->{test_counters}->{command_test}++;
	}

	# repair the command test environment

	$execution_context->ec_end($self, $command_definition);

	# register if this command had side effects

	#! e.g. using a preparer/reparer combination

	$global_previous_command_side_effects ||= $command_definition->{side_effects} || 0;

	# increment command definition test count

	$global_test_report->{global}->{test_counters}->{command_definition}++;
    }

    # repair the test environment

    $execution_context->ec_end($self);

    # register if this command had side effects

    #! don't think this make sense, but anyway ...

    $global_previous_command_side_effects ||= $self->{side_effects} || 0;

    # if library checksum mismatch

    if (defined ($global_config->{model_library}))
    {
	my $library_sha_before = $execution_context->{library_sha_before};

	my $library_sha_after = ModelLibrary::sha();

	if ($library_sha_after ne $library_sha_before)
	{
	    my $error = 'model library checksum mismatch (model library has changed)';

	    my $description = $self->{description};

	    Heterarch::Test::Reporting::report_error_add
		    (
		     $fd_output,
		     {
		      description => $description,
		      error => $error,
		      module_name => $module_name,
		     },
		    );
	}
    }

    # return result: execution completed

    return '';
}


package Heterarch::Test::Output;

#
# A Heterarch::Test::Output::Summarizer implements ->ex_mo_prepare(),
# ex_cd_start() and ex_ct_run() methods that select content and
# prepare a structured for generating a summary in YAML of this
# selection.
#

our @ISA = ("Heterarch::Test::Executor");


package Heterarch::Test::Output::Summarizer;

#
# A Heterarch::Test::Output::Summarizer implements ->ex_mo_prepare(),
# ex_cd_start() and ex_ct_run() methods that select content and
# prepare a structured for generating a summary in YAML of this
# selection.
#

our @ISA = ("Heterarch::Test::Output");


package Heterarch::Test::Output::Formatter;

#
# A Heterarch::Test::Output::Formatter implements ->ex_mo_prepare(),
# ex_cd_start() and ex_ct_run() methods that prepare structured
# content for the output formatter to convert to its target format.
#

our @ISA = ("Heterarch::Test::Output");


#
# Create the infrastructure for keeping module test results.
#

sub ex_mo_prepare
{
    my $self = shift;

    # create an output entry for the module

    my $execution_context = $self->{execution_context};

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    if (not $global_test_report->{selected}->{$module_name})
    {
	my $module_counter = $execution_context->get_module_definition_counter();

	$global_test_report->{selected}->{$module_name}
	    = {
	       count => $module_counter,
	       command_definitions => [],
	       description => $module_definition->{description},
	       documentation => $module_definition->{documentation},
	       harnessing => {
			      preparation => $module_definition->{harnessing}->{preparation}->{description},
			      reparation => $module_definition->{harnessing}->{reparation}->{description},
			     },
	       name => $module_name,
	      };
    }
    else
    {
	die "$0: *** Error: module_name $module_name is defined twice.";
    }
}


#
# Adds the current test description to the summary, constructs a
# structure to keep results.
#

sub ex_ct_run
{
    my $self = shift;

    my $command_test = shift;

    my $execution_context = $self->{execution_context};

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    # get the current module summary

    my $selected_module = $global_test_report->{selected}->{$module_name};

    # get the current test summary

    my $module_command_definitions = $selected_module->{command_definitions};

    my $command_definition_counter = $execution_context->get_command_definition_counter_per_module();

    my $selected_command_definition = $module_command_definitions->[$command_definition_counter];

    # get the current command test

    my $selected_test_definitions = $selected_command_definition->{tests};

    my $test_definition_counter = $execution_context->get_command_test_counter_per_command_definition();

    if (not $selected_test_definitions->[$test_definition_counter])
    {
	$selected_test_definitions->[$test_definition_counter] = {};
    }

    use Clone 'clone';

    $selected_test_definitions->[$test_definition_counter] = clone($command_test);

    # my $selected_test_definition = $selected_test_definitions->[$test_definition_counter];

    # # add the current test description to the summary

    # $selected_test_definition->{description} = $command_test->{description};

    # always succeeds

    my $test_result
	= {
	   after_match => "",
	   before_match => "",
	   error => "",
	   expected => "",
	   matched_pattern_position => "",
	   successfully_matching_string => "",
	  };

    return $test_result;
}


#
# Adds the current command definition to the summary, constructs a
# structure to keep results.
#

sub ex_cd_start
{
    my $self = shift;

    # get the execution context

    my $execution_context = $self->{execution_context};

    # create an output entry for the module

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    # create an output entry for the current command definition

    my $selected_module = $global_test_report->{selected}->{$module_name};

    my $selected_command_definitions = $selected_module->{command_definitions};

    my $command_definition = $execution_context->get_command_definition();

    use Clone 'clone';

#     push @$selected_command_definitions, { description => $command_definition->{description}, tests => [], };

    my $clone_command_definition = clone($command_definition);

    push @$selected_command_definitions, $clone_command_definition;
}


package Heterarch::Test::Output::Formatter::HTMLTable;

# the Formatter:: HTMLTable package converts the test specifications
# to HTML tables.


our @ISA = ("Heterarch::Test::Output::Formatter");


#
# sub start
#
# Incomplete.
#

sub ex_cd_start
{
    my $self = shift;

    my $execution_context = $self->{execution_context};

    my $module_definition = $execution_context->get_module_definition();

    my $command_definition = $execution_context->get_command_definition();

    my $fd_output = $self->{fd_output};

    # we have to either instantiate an object that produces output ...

    if ($command_definition->{class})
    {
	# instantiate the object

	my $class = $command_definition->{class};

	my $filename = $class . ".pm";

	$filename =~ s(::)(/)g;

	# require $filename;

	# $self->{command_object} = eval "$class->new()";
    }

    # ... or run perl code that produces output

    elsif (ref $command_definition->{command} eq 'CODE')
    {
	# code is already instantiated, nothing to do here
    }

    # ... or run a command that produces output

    else
    {
	# start the command and connect with its I/O channels

	# my ($exp, $test_startup) = $command_definition->spawn_new($fd_output);

	my $command = $command_definition->{command};

	# remember to spawn a new command

	my $spawn_new = not $command_definition->{recycle};

	my $arguments = $command_definition->{arguments};

	# if the command can be executed

	if (defined $command)
	{
	    if ($spawn_new)
	    {
	    }
	    else
	    {
		# print $fd_output "*** Recycling $command " . (join ' ', map { "'$_'" } @$arguments) . "\n";
	    }
	}
    }
}


package Heterarch::Test::Output::Formatter::Latex;

# The Heterarch::Test::Output::Formatter::Latex package converts the test specifications
# to a Latex document.


our @ISA = ("Heterarch::Test::Output::Formatter");


our $output_file;


sub _output_latex
{
    my $outputfile = shift;

    my $contents = shift;

    $contents =~ s/_/\\_/g;
    $contents =~ s/&/\\&/g;

    print $outputfile $contents;
}


sub _output_latex_table
{
    my $outputfile = shift;

    my $contents = shift;

    my $prompt = shift;

    _output_latex_environment_prompt
	(
	 $output_file,
	 {
	  content_type => "Verbatim",
	  prompt => $prompt,
	  suffix => "\n\n",
	 },
	);

    _output_latex($output_file, '\begin{longtable}{p{13cm}}' . "\n");

    _output_latex($output_file, '\hline' . "\n");

    _output_latex_environment_prompt
	(
	 $output_file,
	 {
	  content_type => "Verbatim",
	  content => $contents,
	 }
	);

    _output_latex($output_file, '\\\\' . "\n");

    _output_latex($output_file, '\hline' . "\n");

    _output_latex($output_file, '\end{longtable}' . "\n");

}


sub _output_latex_environment_prompt
{
    my $outputfile = shift;

    my $content_options = shift;

    my $content_line_limit = $content_options->{content_line_limit};
    my $content_type = $content_options->{content_type};
    my $content_type_options = $content_options->{content_type_options} || "";
    my $content = $content_options->{content} || "";
    my $prompt = $content_options->{prompt} || "";
    my $suffix = $content_options->{suffix} || "";

    if (defined $content)
    {
	if ($content_line_limit)
	{
	    my $line_count = $content =~ tr/\n//;

	    if ($line_count > $content_line_limit)
	    {
		my $first_lines = join "\n", ( split /\n/, $content )[0 .. $content_line_limit];

		$content = $first_lines . "\n ... < cut at $content_line_limit lines, more follows > ...\n";
	    }
	}

	_output_latex($output_file, $prompt);

	if ($content_type)
	{
	    _output_latex($output_file, '\begin{' . $content_type . "}[$content_type_options]\n");
	}

	_output_latex($output_file, $content);

	if ($content !~ /\n$/)
	{
	    _output_latex($output_file, "\n");
	}

	if ($content_type)
	{
	    _output_latex($output_file, '\end{' . $content_type . "}\n");
	}
    }

    _output_latex($output_file, $suffix);
}


sub _output_latex_postamble
{
    my $postamble
	= '\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
';

#    $output_file = IO::File->new(">>$output_filename");

    print $output_file $postamble;
}


sub _output_latex_preamble
{
    my $output_filename = shift;

    my $html_introduction = shift;

    $output_file = IO::File->new(">$output_filename");

    my $title_paragraphs = _output_latex_preamble_construct($html_introduction);

    my $preamble1
	= '\documentclass[12pt]{article}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage{color}
\usepackage[colorlinks=true]{hyperref}
\usepackage[dvips]{epsfig}
\usepackage{enumitem} % [noitemsep,nolistsep] compact itemize/enumerate etc.
\usepackage{longtable}
% \usepackage[margin=2cm]{geometry}
\usepackage{url}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{relsize}

% Typeface
\usepackage[condensed,math]{iwona}
\usepackage[T1]{fontenc}

';

    print $output_file $preamble1;

    _output_latex_preamble_title($title_paragraphs);

    my $preamble2
	= '
\begin{document}

\maketitle

';

    print $output_file $preamble2;

    _output_latex_preamble_abstract($title_paragraphs);

    _output_latex_preamble_summary($title_paragraphs);
}


sub _output_latex_preamble_abstract
{
    my $title_paragraphs = shift;

}


sub _output_latex_preamble_construct
{
    my $html_introduction_original = shift;

    my $html_introduction = $html_introduction_original;

    # extract email address

    $html_introduction =~ m(<address>(.*)</address>)gs;

    my $email_anchored = $1;

    my $author = $email_anchored || "";

    $author =~ m(<a.*?>(.*)</a>)gs;

    $author = $1 || "";

    my $thanks = $email_anchored || "";

    $thanks =~ m("mailto:(.*)")gs;

    $thanks = $1 || "";

    # remove trailing part

    $html_introduction =~ s(<address>(.*))()gs;

    # extract title

    $html_introduction =~ s(<h1>(.*)</h1>)()gs;

    my $title = $1 || "";

    # remove irrelevant html tags

    $html_introduction =~ s(<p/?>)()gs;

    $html_introduction =~ s(<hr/?>)()gs;

    # split in paragraphs

    my $paragraphs = [ split "\n\n", $html_introduction, ];

    # construct and return result

    my $result
	= {
	   author => $author,
	   email_anchored => $email_anchored,
	   original_contents => $html_introduction_original,
	   paragraphs => $paragraphs,
	   thanks => $thanks,
	   title => $title,
	  };

    return $result
}


sub _output_latex_preamble_summary
{
    my $title_paragraphs = shift;

    my $paragraphs = $title_paragraphs->{paragraphs};

    if (scalar @$paragraphs)
    {
	# output the description as a section title

	_output_latex($output_file, '\section*{\center ' . "Summary" . "}\n\n");

	# write paragraphs

	foreach my $paragraph (@$paragraphs)
	{
	    _output_latex($output_file, "$paragraph\n\n");
	}
    }
}


sub _output_latex_preamble_title
{
    my $title_paragraphs = shift;

    # write title

    _output_latex($output_file, '\title{' . $title_paragraphs->{title} . "}\n\n");

    _output_latex($output_file, '\date{' . localtime() . "}\n\n");

    _output_latex($output_file, '\author{' . $title_paragraphs->{author} . "}\n\n");

}


sub _output_latex_process
{
    my $test_report = shift;

    my $module_definitions = $test_report->{selected};

    foreach my $module_definition_key (sort
				       {
					   my $selected_a = $a;
					   my $selected_b = $b;

					   my $counter_a = $module_definitions->{$selected_a}->{count};
					   my $counter_b = $module_definitions->{$selected_b}->{count};

					   $counter_a <=> $counter_b;
				       }
				       keys %$module_definitions)
    {
	my $module_definition = $module_definitions->{$module_definition_key};

	my $module_definition_description = $module_definition->{description};

	my $module_definition_documentation = $module_definition->{documentation} || "";

	my $command_definitions = $module_definition->{command_definitions};

	# output the description as a section title

	_output_latex($output_file, '\section{' . ucfirst $module_definition_description . "}\n\n");

	# output the filename

	if (scalar @$command_definitions eq 0)
	{
	    _output_latex($output_file, 'File ');

	    _output_latex_prompt($output_file, "textit", $module_definition->{name}, "", "");

	    if ($module_definition_documentation)
	    {
		_output_latex($output_file, ':  ');
	    }
	    else
	    {
		_output_latex($output_file, '.');
	    }
	}
	elsif (scalar @$command_definitions eq 1)
	{
	    _output_latex($output_file, 'The file ');

	    _output_latex_prompt($output_file, "textit", $module_definition->{name}, "", "");

	    _output_latex($output_file, " defines the test that is explained in the following section.\n\n");
	}
	else
	{
	    _output_latex($output_file, 'The file ');

	    _output_latex_prompt($output_file, "textit", $module_definition->{name}, "", "");

	    _output_latex($output_file, " defines the tests that are explained in the following sections.\n\n");
	}

	# output the documentation and explanation

	if ($module_definition_documentation)
	{
	    _output_latex($output_file, "$module_definition_documentation->{purpose}\n\n");

	    _output_latex($output_file, "$module_definition_documentation->{explanation}\n\n");

	    # _output_latex($output_file, "Test commands follow.\n\n");
	}
	else
	{
	    _output_latex($output_file, "\n\n");
	}

	# output the preparation and reparation

	if (scalar @$command_definitions ne 0)
	{
	    my $harnessing = $module_definition->{harnessing};

	    if ($harnessing->{preparation})
	    {
		_output_latex($output_file, '\begin{' . "itemize" . "}\n");

		_output_latex($output_file, '\item ');

		_output_latex_prompt($output_file, "textit", $module_definition->{harnessing}->{preparation}, "Preparation: ", "\n");

		_output_latex($output_file, '\item ');

		_output_latex_prompt($output_file, "textit", $module_definition->{harnessing}->{reparation}, "Reparation: ", "\n");

		_output_latex($output_file, '\end{' . "itemize" . "}\n");
	    }
	}

	# output the command definitions

	foreach my $command_definition (@$command_definitions)
	{
	    my $description = $command_definition->{description};

	    my $command_tests = $command_definition->{command_tests};

	    # output the command and its arguments

	    my $arguments = $command_definition->{arguments};

	    my $command = join ' ', $command_definition->{command}, @$arguments;

	    if (scalar @$command_definitions eq 1)
	    {
		_output_latex($output_file, "The following test is described as: $description\n");
	    }
	    else
	    {
		if (scalar @$command_tests eq 1)
		{
		    my $subdescription = $command_tests->[0]->{description};

		    $description .= ": $subdescription";
		}

		_output_latex($output_file, '\subsection{' . ucfirst $description . "}\n\n");
	    }

	    if (scalar @$command_tests eq 1)
	    {
		_output_latex($output_file, 'The test is started with system shell command: ');
	    }
	    else
	    {
		_output_latex($output_file, 'The tests are started with the system shell command: ');
	    }

	    _output_latex_prompt($output_file, "textbf", $command, "", "");

	    _output_latex_prompt($output_file, "", $command_definition->{comment}, "Notes:\n", "\n\n");

	    # output the preparation and reparation

	    my $harnessing = $command_definition->{harnessing};

	    if ($harnessing->{preparation})
	    {
		_output_latex($output_file, '\begin{' . "itemize" . "}\n");

		_output_latex($output_file, '\item ');

		_output_latex_prompt($output_file, "textit", $module_definition->{harnessing}->{preparation}, "Preparation: ", "\n");

		_output_latex($output_file, '\item ');

		_output_latex_prompt($output_file, "textit", $module_definition->{harnessing}->{reparation}, "Reparation: ", "\n");

		_output_latex($output_file, '\end{' . "itemize" . "}\n");
	    }

	    foreach my $command_test (@$command_tests)
	    {
		_output_latex_prompt($output_file, "textbf", $command_test->{disabled}, "note that this test was disabled: \n", "\n\n");

		my $description = $command_test->{description};

		if (scalar @$command_tests eq 1)
		{
		    # _output_latex($output_file, "This test is described as: $description\n\n");
		}
		else
		{
		    _output_latex($output_file, '\subsubsection{' . ucfirst $description . "}\n\n");
		}

		my $write = $command_test->{write} || "";

		if ($write)
		{
		    _output_latex_environment_prompt
			(
			 $output_file,
			 {
			  content => $write,
			  content_line_limit => 10,
			  content_type => "Verbatim",
			  content_type_options => 'fontsize=\relsize{-2}',
			  prompt => "The input to the application is:\n",
			  suffix => "\n\n",
			 },
			);
		}

		# 		_output_latex_table($output_file, $command_test->{read}, "This is the application output we expect:\n");

		my $read = $command_test->{read} || "";

		if ($read)
		{
		    _output_latex_environment_prompt
			(
			 $output_file,
			 {
			  content => $read,
			  content_line_limit => 10,
			  content_type => "Verbatim",
			  content_type_options => 'fontsize=\relsize{-2}',
			  prompt => "The application output we expect:\n",
			  suffix => "\n\n",
			 },
			);
		}

		_output_latex_prompt($output_file, "", $command_test->{comment}, "Notes:\n", "\n\n");

	    }
	}
    }
}


sub _output_latex_prompt
{
    my $outputfile = shift;

    my $content_type = shift;

    my $contents = shift;

    my $prompt = shift;

    my $suffix = shift;

    if (defined $contents)
    {
	_output_latex($output_file, $prompt);

	if ($content_type)
	{
	    _output_latex($output_file, '\\' . $content_type . '{');
	}

	_output_latex($output_file, $contents);

	if ($content_type)
	{
	    _output_latex($output_file, "}");
	}
    }

    _output_latex($output_file, $suffix);
}


sub process
{
    my $output_filename = shift;

    my $test_module_library_contents = shift;

    my $global_test_report = shift;

    _output_latex_preamble
	(
	 $output_filename,
	 $test_module_library_contents->{html_introduction},
	);

    _output_latex_process($global_test_report);

    _output_latex_postamble();
}


package Heterarch::Test::Output::Formatter::YAML::Summary;

# The Formatter:YAML::Summary package converts the test specifications
# to a YAML formatted summary.


our @ISA = ("Heterarch::Test::Output::Summarizer");


#
# Create the infrastructure for keeping module test results.
#

sub ex_mo_prepare
{
    my $self = shift;

    # get the execution context

    my $execution_context = $self->{execution_context};

    # create an output entry for the module

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    if (not $global_test_report->{selected}->{$module_name})
    {
	$global_test_report->{selected}->{$module_name} = [];
    }
    else
    {
	die "$0: *** Error: module_name $module_name is defined twice.";
    }
}


# add the current test description to the summary

sub ex_ct_run
{
    my $self = shift;

    my $command_test = shift;

    my $execution_context = $self->{execution_context};

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    # get the current module summary

    my $selected_module = $global_test_report->{selected}->{$module_name};

    # get the current test summary

    my $selected_command_definitions = $selected_module->[$#$selected_module]->{tests};

    # add the current test description to the summary

    push @$selected_command_definitions, $command_test->{description};

    # always succeeds

    my $test_result
	= {
	   after_match => "",
	   before_match => "",
	   error => "",
	   expected => "",
	   matched_pattern_position => "",
	   successfully_matching_string => "",
	  };

    return $test_result;
}


# create an output entry for the current command definition

sub ex_cd_start
{
    my $self = shift;

    # get the execution context

    my $execution_context = $self->{execution_context};

    # create an output entry for the module

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    # create an output entry for the current command definition

    my $selected_module = $global_test_report->{selected}->{$module_name};

    my $command_definition = $execution_context->get_command_definition();

    push @$selected_module, { description => $command_definition->{description}, tests => [], };
}


package Heterarch::Test::Output::Formatter::YAML::CommandTests;

# The Formatter:YAML::CommandTests package converts the test
# specifications to a YAML formatted list of commands and their input
# in the write clauses, that would be executed if the tests are run.


our @ISA = ("Heterarch::Test::Output::Summarizer");


#
# Create the infrastructure for keeping module test results.
#

sub ex_mo_prepare
{
    my $self = shift;

}


# create an output entry for the current command definition
# add the current test description to the summary

sub ex_ct_run
{
    my $self = shift;

    my $command_test = shift;

    my $execution_context = $self->{execution_context};

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    # only if there is something to add

    if (exists $command_test->{write}
        and defined $command_test->{write})
    {
	# get the current module summary

	my $selected_module = $global_test_report->{selected}->{$module_name};

	# get the current test summary

	if (not exists $selected_module->[$#$selected_module]->{tests})
	{
	    $selected_module->[$#$selected_module]->{tests} = [];
	}

	my $selected_command_definitions = $selected_module->[$#$selected_module]->{tests};

	# add the current test write clause to the summary

	push @$selected_command_definitions, $command_test->{write};
    }

    # always succeeds

    my $test_result
	= {
	   after_match => "",
	   before_match => "",
	   error => "",
	   expected => "",
	   matched_pattern_position => "",
	   successfully_matching_string => "",
	  };

    return $test_result;
}


sub ex_cd_start
{
    my $self = shift;

    # get the execution context

    my $execution_context = $self->{execution_context};

    # create an output entry for the module

    #t we could add a counter here to keep the order of execution in the final yaml output

    my $module_definition = $execution_context->get_module_definition();

    my $module_name = $module_definition->{name};

    my $command_definition = $execution_context->get_command_definition();

    if (exists $command_definition->{command})
    {
	if (not $global_test_report->{selected}->{$module_name})
	{
	    $global_test_report->{selected}->{$module_name} = [];
	}

	# create an output entry for the current command definition

	my $selected_module = $global_test_report->{selected}->{$module_name};

	my $command = $command_definition->{command};

	my $arguments = $command_definition->{arguments};

	my $command_string = $command . ' ' . join ' ', @ { $arguments || [] };

	push @$selected_module, { command => $command_string, };
    }
}


package Heterarch::Test::Output::Definitions;

# The Heterarch::Test::Output::Definitions package should list all the
# supported constructs that can be used in the test specifications.
#
# This list has been developed in a separate emacs-org document.
#

our @ISA = ("Heterarch::Test::Output::Summarizer");


package Heterarch::Test::Output::List;

# this package should list the constructs used in the selected test
# specifications.

# inherit from the yaml summary to automate text context tracking through its start sub

our @ISA = ("Heterarch::Test::Output::Formatter::YAML::Summary");


sub ex_ct_run
{
    my $self = shift;

    my $command_test = shift;

    my $execution_context = $self->{execution_context};

    my $command_definition = $execution_context->get_command_definition();

    my $module_definition = $execution_context->get_module_definition();

}


package Heterarch::Test::Reporting;


sub report_error_add
{
    my $fd_output = shift;

    my $arguments = shift;

    my $description = $arguments->{description};
    my $error = $arguments->{error};
    my $expected = $arguments->{expected};
    my $message = $arguments->{message};
    my $module_name = $arguments->{module_name};
    my $seen = $arguments->{seen};
    my $subdescription = $arguments->{subdescription};

    my $package_name = $global_test_report->{description}->{package}->{name};

    $global_error_count++;

    print $fd_output "*** Error $global_error_count: $error ($description, package $package_name, $module_name, error_count $global_error_count)\n";

    # fill in the error report

    $global_test_report->{global}->{error_count} = $global_error_count;

    if ($subdescription)
    {
	$global_test_report->{errors}->{modules}->{$module_name}->{$global_error_count}->{$subdescription}->{description} = $description;
    }
    else
    {
	$global_test_report->{errors}->{modules}->{$module_name}->{$global_error_count}->{description} = $description;
    }

    $global_test_report->{errors}->{modules}->{$module_name}->{$global_error_count}->{error} = $error;

    if (defined $message)
    {
	#! subdescription can be undefined here, in which case
	#! $message is undefined too.  Both can be defined when
	#! running in verbose mode seems, allows to track regex
	#! problems ... I think.

	$global_test_report->{errors}->{modules}->{$module_name}->{$global_error_count}->{$subdescription}->{report} = $message;
    }

    if (defined $expected
	and defined $seen)
    {
	my $processed_expected = [ split "\n", $expected, ];

	$processed_expected = join "\n*** Error $global_error_count: expected: ", "", @$processed_expected, "\n";

	print $fd_output $processed_expected;

	my $processed_seen = [ split "\n", $seen, ];

	$processed_seen = join "\n*** Error $global_error_count: seen: ", "", @$processed_seen, "\n";

	print $fd_output $processed_seen;

	use IO::File;

	my $expected_filename = "/tmp/text_$global_config->{package}->{name}_$global_error_count.expected";

	my $expected_file = IO::File->new(">$expected_filename");

	if ($expected_file)
	{
	    print $expected_file $expected;

	    $expected_file->close();
	}
	else
	{
	    print $fd_output "*** Warning: cannot open $expected_filename for writing\n";
	}

	my $seen_filename = "/tmp/text_$global_config->{package}->{name}_$global_error_count.seen";

	my $seen_file = IO::File->new(">$seen_filename");

	if ($seen_filename)
	{
	    print $seen_file $seen;

	    $seen_file->close();
	}
	else
	{
	    print $fd_output "*** Warning: cannot open $seen_filename for writing\n";
	}

	my $diff = `diff "$expected_filename" "$seen_filename"`;

	my $diff_filename = "/tmp/text_$global_config->{package}->{name}_$global_error_count.diff";

	my $diff_file = IO::File->new(">$diff_filename");

	if ($diff_file)
	{
	    print $diff_file $diff;

	    $diff_file->close();
	}
	else
	{
	    print $fd_output "*** Warning: cannot open $diff_filename for writing\n";
	}

	my $processed_diff = [ split "\n", $diff, ];

	$processed_diff = join "\n*** Error $global_error_count: diff: ", "", @$processed_diff, "\n";

	print $fd_output $processed_diff;
    }

}


my $rulers_not_used
    = {
       end => { bottom => 2, top => 1, },
       info => { bottom => 1, top => 1, },
       start => { bottom => 1, top => 2, },
      };

sub report_message
{
    my $fd_output = shift;

    my $message = shift;

    print $fd_output "$message\n";
}


sub report_message_end
{
    my $fd_output = shift;

    my $message = shift;

    return _report_message_with_rulers($fd_output, 1, 2, $message);
}


sub report_message_info
{
    my $fd_output = shift;

    my $message = shift;

    return _report_message_with_rulers($fd_output, 1, 1, $message);
}


sub report_message_start
{
    my $fd_output = shift;

    my $message = shift;

    return _report_message_with_rulers($fd_output, 2, 1, $message);
}


sub _report_message_with_rulers
{
    my $fd_output = shift;

    my $top_ruler = shift;

    my $bottom_ruler = shift;

    my $message = shift;

    my $lines = [ split '\n', $message, ];

    my $longest = 0;

    map
    {
	($longest < length) && ($longest = length)
    }
	@$lines;

    my $line = '-' x $longest;

    print $fd_output "\n";
    print $fd_output "$line\n" for 0 .. $top_ruler;
    print $fd_output "\n";
    print $fd_output "$message\n\n";
    print $fd_output "$line\n" for 0 .. $bottom_ruler;
    print $fd_output "\n";
}


package main;


main();


